{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone Report\n",
    "\n",
    "Disclosure: I have very little affinity with R and I will probably not use it in the future. Teachers are asking us to take on something we didn't learn before (NLP) and like I prefere Python I will use the Spacy library and other technologies for this capstone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk import bigrams, trigrams\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en') # loading the english model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "We are going to focus on the english datasets. As the structure of those files are all the same (just lines of text) let's concatenate them in a single file named `corpus.txt` using the following command lines:\n",
    "\n",
    "```shell\n",
    "> cat en_US.*.txt > corpus.txt\n",
    "> wc -l corpus.txt \n",
    " 4269678 corpus.txt\n",
    "```\n",
    "\n",
    "The file can be found in the project at `datasets/corpus.txt`\n",
    "\n",
    "We probably only need words so let's keep only alpha tokens in lowercase. \n",
    "\n",
    "We will use [spacy](https://spacy.io/) for the purpose. Like, it takes resources I will store the result in a new file named `preprocessed_corpus.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('datasets/preprocessed_corpus.txt'):\n",
    "    with open('datasets/corpus.txt') as corpus:\n",
    "        with open('datasets/preprocessed_corpus.txt', 'w') as preprocessed_corpus:\n",
    "            for line in corpus.readlines():\n",
    "                preprocessed_corpus.write('{}\\n'.format(','.join([token.lower_ for token in nlp(line.strip()) if token.is_alpha])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monogram frequencies\n",
    "As our `preprocessed_corpus.csv` contains more than 4 millions lines we do not want to extract the monogram frequencies so we are going to store this result in a new dataset named `monogram_frequencies.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('datasets/monogram_frequencies.txt'):\n",
    "    words_count = Counter()\n",
    "    for line in open('datasets/preprocessed_corpus.txt').readlines():\n",
    "        words_count += Counter(line.strip().split(','))\n",
    "        \n",
    "    with open('datasets/monogram_frequencies.txt', 'w') as f:\n",
    "        for word, count in words_count.most_common():\n",
    "            f.write('{},{}\\n'.format(word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is just a naive approach and a more down to earth is:\n",
    " - split the data with command line `split -l 711613 preprocessed_corpus.txt`\n",
    " - run the python script provided in the repository: `python monogram_counter.py datasets/xaa` for each `datasets/xa?`\n",
    " - aggregate all the results: `python monogram_counter_aggregator.py datasets/xa*.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis\n",
    "> TODO try to calculate coverage for monogram, bigram and trigram for each quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single unique words count 0\n"
     ]
    }
   ],
   "source": [
    "words_count = Counter()\n",
    "# for line in open('datasets/en_US/preprocessed_corpus.txt').readlines():\n",
    "#      words_count += Counter(line.strip().split(','))\n",
    "\n",
    "# list(bigrams( ['i', 'am', 'going', 'to', 'the', 'cinema']))\n",
    "# list(trigrams( ['i', 'am', 'going', 'to', 'the', 'cinema']))\n",
    "\n",
    "print('Single unique words count {}'.format(len(words_count) ))\n",
    "def calculate_coverage(wc):\n",
    "    wc.most_common(200)\n",
    "    total_nb_words = sum(wc.values())\n",
    "    fifty_percent_words = total_nb_words * 0.5\n",
    "    ninety_percent_words = total_nb_words * 0.9\n",
    "    fifty_percent_count =  0\n",
    "    ninety_percent_count =  0\n",
    "    fifty_percent_unique_words =  0\n",
    "    ninety_percent_unique_words =  0\n",
    "\n",
    "    for i, (word, count) in enumerate(wc.most_common()):\n",
    "        if fifty_percent_count < fifty_percent_words:\n",
    "            fifty_percent_count += count\n",
    "            if fifty_percent_count >= fifty_percent_words:\n",
    "                fifty_percent_unique_words = i + 1\n",
    "            \n",
    "        if ninety_percent_count < ninety_percent_words:\n",
    "            ninety_percent_count += count\n",
    "        else:\n",
    "            ninety_percent_unique_words = i + 1\n",
    "    \n",
    "    return (fifty_percent_unique_words, ninety_percent_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you evaluate how many of the words come from foreign languages?\n",
    "- Use a static dictionary\n",
    "- Test if the characters belong the the alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
